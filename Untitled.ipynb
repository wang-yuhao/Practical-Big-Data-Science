{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wikidata.entity.Entity Q20145 'IU'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wikidata.client import Client\n",
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)\n",
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m'South Korean singer and actress'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wikidata.commonsmedia.File 'File:IU performing at her 24 Steps Concert in Seoul (2016).jpg'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_prop = client.get('P18')\n",
    "image = entity[image_prop]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1484, 2230)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.image_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://upload.wikimedia.org/wikipedia/commons/3/38/IU_performing_at_her_24_Steps_Concert_in_Seoul_%282016%29.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time \n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path\n",
    "import math\n",
    "import timeit\n",
    "from multiprocessing import JoinableQueue, Queue, Process\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fc839b02fac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "class TransE:\n",
    "        @property\n",
    "        def variables(self):\n",
    "                return self.__variables\n",
    "        \n",
    "        @property\n",
    "        def num_triple_train(self):\n",
    "                return self.__num_triple_train\n",
    "            \n",
    "        @property\n",
    "        def num_triple_test(self):\n",
    "                return self.__num_triple_test\n",
    "            \n",
    "        @property\n",
    "        def testing_data(self):\n",
    "                return self.__triple_test\n",
    "            \n",
    "        @property\n",
    "        def num_entity(self):\n",
    "                return self.__num_entity\n",
    "            \n",
    "        @property\n",
    "        def embedding_entity(self):\n",
    "                return self.__embedding_entity\n",
    "            \n",
    "        @property\n",
    "        def embedding_relation(self):\n",
    "                return self.__embedding_relation\n",
    "            \n",
    "        @property\n",
    "        def hr_t(self):\n",
    "                return self.__hr_t\n",
    "            \n",
    "        @property\n",
    "        def tr_h(self):\n",
    "                return self.__tr_h\n",
    "            \n",
    "\n",
    "\n",
    "        def training_data_batch(self,batch_size = 512):\n",
    "            n_triple = len\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self):\n",
    "        print('loading entity2id ...')\n",
    "        with open(os.path.join(self.__data_dir, 'entity2id.txt')) as f:\n",
    "                self.__entity2id = {line.strip().split()('\\t')[0]: int(line.strip().split('\\t')[1]) for line in f.readlines()}\n",
    "                self.__id2entity = {value:key for key,value in self.__entity2id.items()}\n",
    "\n",
    "        with open(os.path.join(self.__data_dir,'relation2id.txt')) as f:\n",
    "                self.__relation2id = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[1]) for line in f.readlines()}\n",
    "                self.__id2relation = {value:key for key, value in self.__relation2id.items()}\\\n",
    "                \n",
    "        def load_triple(self,triplefile):\n",
    "                    triple_list = []\n",
    "                    with open(os.path.join(self.__data_dir,triplefile)) as f:\n",
    "                            for line in f.readlines():\n",
    "                                    line_list = line.strip().split('\\t')\n",
    "                                    assert len(line_list) == 3\n",
    "                                    headid = self.__entity2id[line_list[0]]\n",
    "                                    relationid = self.__relation2id[line_list[1]]\n",
    "                                    tailid = self.__relation2id[line_list[2]]\n",
    "                                    triple_list.append((headid,relationid,tailid))\n",
    "                                    self.__hr_t[(headid,relationid)].add(tailid)\n",
    "                                    self.__tr_h[(tailid,relationid)].add(headid)\n",
    "                    return triple_list\n",
    "                \n",
    "        self.__hr_t = defaultdict(set)\n",
    "        self.__tr_h = defaultdict(set)\n",
    "        self.__triple_train = load_triple(self, 'train.txt')\n",
    "        self.__triple_test = load_triple(self, 'test.txt')\n",
    "        self.__triple_valid = load_triple(self, 'valid.txt')\n",
    "        self.__triple = np.concatenate([self.__triple_train,self.__triple_test, self.triple_valid], axis = 0)\n",
    "        \n",
    "        self.__num_relation = len(self.__relation2id)\n",
    "        self.__num_entity = len(self.entity2id)\n",
    "        self.__num_triple_train = len(self.__triple_train)\n",
    "        self.__num_triple_test = len(self,__triple_test)\n",
    "        self.__num_triple_valid = len(self.__triple_valid)\n",
    "        \n",
    "        print('entity number:' + str(self.__num_entity))\n",
    "        print('relation number: ' + str(self.__num_relation))\n",
    "        print('training triple number: ' + str(self.__num_triple_train))\n",
    "        print('testing triple number: ' + str(self.__num_triple_test))\n",
    "        print('valid triple number: ' + str(self.__num_triple_valid))\n",
    "        \n",
    "        if self.__negative_sampling == 'bern':\n",
    "                self.__relation_property_head = {x:[] for x in range(self.__num_relation)} #{relation_id:[headid1, headid2,...]}\n",
    "                self.__relation_property_tail = {x:[] for x in range(self.__num_relation)} #{relation_id:[tailid1, tailid2,...]}\n",
    "                for t in self.__triple_train:\n",
    "                    #print(t)\n",
    "                    self.__relation_property_head[t[1]].append(t[0])\n",
    "                    self.__relation_property_tail[t[1]].append(t[2])\n",
    "                self.__relation_property = {x:(len(set(self.__relation_property_tail[x])))/(len(set(self.__relation_property_head[x]))+ len(set(self.__relation_property_tail[x]))) \\\n",
    "                                             for x in self.__relation_property_head.keys()} # {relation_id: p, ...} 0< num <1, and for relation replace head entity with the property p\n",
    "        else: \n",
    "            print(\"unif set do'n need to calculate hpt and tph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, data_dir, negative_sampling,learning_rate, \n",
    "\t\t\t\t batch_size, max_iter, margin, dimension, norm, evaluation_size, regularizer_weight):\n",
    "\t\t# this part for data prepare\n",
    "\t\tself.__data_dir=data_dir\n",
    "\t\tself.__negative_sampling=negative_sampling\n",
    "\t\tself.__regularizer_weight = regularizer_weight\n",
    "\t\tself.__norm = norm\n",
    "\n",
    "\t\tself.__entity2id={}\n",
    "\t\tself.__id2entity={}\n",
    "\t\tself.__relation2id={}\n",
    "\t\tself.__id2relation={}\n",
    "\n",
    "\t\tself.__triple_train=[] #[(head_id, relation_id, tail_id),...]\n",
    "\t\tself.__triple_test=[]\n",
    "\t\tself.__triple_valid=[]\n",
    "\t\tself.__triple = []\n",
    "\n",
    "\t\tself.__num_entity=0\n",
    "\t\tself.__num_relation=0\n",
    "\t\tself.__num_triple_train=0\n",
    "\t\tself.__num_triple_test=0\n",
    "\t\tself.__num_triple_valid=0\n",
    "\n",
    "\t\t# load all the file: entity2id.txt, relation2id.txt, train.txt, test.txt, valid.txt\n",
    "\t\tself.load_data()\n",
    "\t\tprint('finish preparing data. ')\n",
    "\n",
    "\n",
    "\t\t# this part for the model:\n",
    "\t\tself.__learning_rate = learning_rate\n",
    "\t\tself.__batch_size = batch_size\n",
    "\t\tself.__max_iter = max_iter\n",
    "\t\tself.__margin = margin\n",
    "\t\tself.__dimension = dimension\n",
    "\t\tself.__variables= []\n",
    "\t\t#self.__norm = norm\n",
    "\t\tself.__evaluation_size = evaluation_size\n",
    "\t\tbound = 6 / math.sqrt(self.__dimension)\n",
    "\t\twith tf.device('/cpu'):\n",
    "\t\t\tself.__embedding_entity = tf.get_variable('embedding_entity', [self.__num_entity, self.__dimension],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   initializer=tf.random_uniform_initializer(minval=-bound, maxval=bound, seed = 123))\n",
    "\t\t\tself.__embedding_relation = tf.get_variable('embedding_relation', [self.__num_relation, self.__dimension],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t initializer=tf.random_uniform_initializer(minval=-bound, maxval=bound, seed =124))\n",
    "\t\t\tself.__variables.append(self.__embedding_entity)\n",
    "\t\t\tself.__variables.append(self.__embedding_relation)\n",
    "\t\t\tprint('finishing initializing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 11 required positional arguments: 'self', 'data_dir', 'negative_sampling', 'learning_rate', 'batch_size', 'max_iter', 'margin', 'dimension', 'norm', 'evaluation_size', and 'regularizer_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-6710ec543692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 11 required positional arguments: 'self', 'data_dir', 'negative_sampling', 'learning_rate', 'batch_size', 'max_iter', 'margin', 'dimension', 'norm', 'evaluation_size', and 'regularizer_weight'"
     ]
    }
   ],
   "source": [
    "__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-3-461d0779fb02>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-461d0779fb02>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    parser.add_argument('--max_iter', dest='max_iter', type=int, help='maximum interation', default=100)\u001b[0m\n\u001b[0m                                                                                                        ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description = \"TransE\")\n",
    "    parser.add_argument('--data_dir', dest='data_dir', type=str, help='the directory of dataset', default='../Fb15k_withtext/')\n",
    "    parser.add_argument('--learning_rate', dest='learning_rate', type=float, help='learning rate', default=0.01)\n",
    "    parser.add_argument('--batch_size', dest='batch_size', type=int, help=\"batch size\", default=4096)\n",
    "\tparser.add_argument('--max_iter', dest='max_iter', type=int, help='maximum interation', default=100)\n",
    "\tparser.add_argument('--optimizer', dest='optimizer', type=str, help='optimizer', default='adam')\n",
    "\tparser.add_argument('--dimension', dest='dimension', type=int, help='embedding dimension', default=50)\n",
    "\tparser.add_argument('--margin', dest='margin', type=float, help='margin', default=1.0)\n",
    "\tparser.add_argument('--norm', dest='norm', type=str, help='L1 or L2 norm', default='L1')\n",
    "\tparser.add_argument('--evaluation_size', dest='evaluation_size', type=int, help='batchsize for evaluation', default=500)\n",
    "\tparser.add_argument('--save_dir', dest='save_dir', type=str, help='directory to save tensorflow checkpoint directory', default='output/')\n",
    "\tparser.add_argument('--negative_sampling', dest='negative_sampling', type=str, help='choose unit or bern to generate negative examples', default='bern')\n",
    "\tparser.add_argument('--evaluate_per_iteration', dest='evaluate_per_iteration', type=int, help='evaluate the training result per x iteration', default=10)\n",
    "\tparser.add_argument('--evaluate_worker', dest='evaluate_worker', type=int, help='number of evaluate workers', default=4)\n",
    "\tparser.add_argument('--regularizer_weight', dest='regularizer_weight', type=float, help='regularization weight', default=1e-5)\n",
    "\tparser.add_argument('--n_test', dest = 'n_test', type=int, help='number of triples for test during the training', default = 300)\n",
    "\targs = parser.parse_args()\n",
    "    print(args)\n",
    "    model = TransE(negative_sampling=args.negative_sampling, data_dir=args.data_dir,\n",
    "                   learning_rate=args.learning_rate, batch_size=args.batch_size, max_iter=args.max_iter, \n",
    "                   margin=args.margin, dimension=args.dimension, norm=args.norm, evaluation_size=args.evaluation_size,\n",
    "                   regularizer_weight=args.regularizer_weight)\n",
    "    \n",
    "    train_triple_positive_input, train_triple_negative_input, loss, op_train, loss_every, norm_entity = train_operation(mode, learning_rate = args.learning_rate, margin = args.margin, optimizer_str = args.optimizer)\n",
    "    \n",
    "    test_triple, head_rank, tail_rank, norm_head_rank, norm_tail_rank = test_operation(model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.init_ops.RandomUniform object at 0x7f9508401198>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "initializer=tf.random_uniform_initializer(minval=-10, maxval=10, seed =124)\n",
    "print(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_operation(model,learning_rate=0.01,margin=1.0, optimizer_str = 'gradient'):\n",
    "        with tf.device('/cpu'):\n",
    "            \n",
    "                #preprae argument which is int32 and row is 3, columns is uncertain\n",
    "                train_triple_positive_input = tf.placeholder(tf.int32,[None,3])\n",
    "                train_triple_negative_input = tf.placeholder(tf.int32,[None,3]) \n",
    "                \n",
    "                loss, loss_every, norm_entity = model.train([train_triple_positive_input,train_triple_negative_input])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "def train(self,inputs):\n",
    "    # embedding_relation,embedding_entity are tensors that row is __num_entity(__num_relation) and column is __dimension\n",
    "    # and randomly distributed\n",
    "    embedding_relation = self.__embedding_relation\n",
    "    embedding_entity = self.__embedding_entity\n",
    "    \n",
    "    triple_positive, triple_negative = inputs #triple_positive:(head_id,relation_id,tail_id\n",
    "    \n",
    "    norm_entity = tf.nn.l2_normalize(embedding_entity,dim = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
